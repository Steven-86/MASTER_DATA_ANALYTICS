!wget https://proai-datasets.s3.eu-west-3.amazonaws.com/bitcoin_tweets.csv

import pandas as pd
dataset = pd.read_csv('/databricks/driver/bitcoin_tweets.csv', delimiter=",")

spark_df = spark.createDataFrame(dataset)
spark_df.write.saveAsTable("bitcoin_tweets")

display(spark_df)

# elimino tutti i gli a capo e i nuova riga all'interno di text, oltre che punteggiatura infine elimino anche numeri e indirizzi web perchè non servono per una analisi del sentiment
from pyspark.sql.functions import regexp_replace

spark_df=spark_df.withColumn('text',regexp_replace('text',r'[\n\r]',' '))
spark_df = spark_df.withColumn("text", regexp_replace('text', r'\b\d+\b', ''))
spark_df = spark_df.withColumn("text", regexp_replace('text', r'http\S+', ''))
#spark_df = spark_df.withColumn("text", regexp_replace('text', r'[^\w\s]', ''))


display(spark_df)

# traduco tutti i tweet in inglese per effettuare poi sentiment analysis

pip install deep-translator

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from deep_translator import GoogleTranslator

# Creare una sessione Spark
spark = SparkSession.builder.appName("Traduzione").getOrCreate()

# Funzione per tradurre il testo utilizzando deep_translator
def translate_text(text):
    return GoogleTranslator(source='auto', target='en').translate(text)

# Creare una UDF (User Defined Function) per la traduzione
translate_udf = udf(translate_text, StringType())

# Applicare la UDF al DataFrame
spark_df = spark_df.withColumn("text_translated", translate_udf(spark_df["text"]))

# Mostrare il DataFrame risultante
spark_df.show(truncate=False)

# applico tecniche di nlp per ottimizzare la raccolta informazioni

pip install nltk

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')

from pyspark.sql.types import ArrayType, StringType
from pyspark.ml.feature import StopWordsRemover
from pyspark.ml.feature import Tokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

import nltk
# Creare una sessione Spark
spark = SparkSession.builder.appName("NLP").getOrCreate()


# Tokenizzare il testo
tokenizer = Tokenizer(inputCol="text_translated", outputCol="words")
wordsData = tokenizer.transform(spark_df)

# Rimuovere le stopwords
remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")
wordsData = remover.transform(wordsData)

# Funzione per lemmatizzare le parole
lemmatizer = WordNetLemmatizer()
def lemmatize_words(words):
    return [lemmatizer.lemmatize(word) for word in words]

# Creare una UDF (User Defined Function) per la lemmatizzazione
lemmatize_udf = udf(lemmatize_words, ArrayType(StringType()))

# Applicare la UDF al DataFrame
spark_df = wordsData.withColumn("lemmatized_words", lemmatize_udf(wordsData["filtered_words"]))

# Mostrare il DataFrame risultante
spark_df.select("text", "lemmatized_words").show(truncate=False)

import nltk
nltk.download('vader_lexicon')

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import FloatType
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Creare una sessione Spark
spark = SparkSession.builder.appName("SentimentAnalysis").getOrCreate()

# Inizializzare il Sentiment Analyzer
sia = SentimentIntensityAnalyzer()

# Funzione per calcolare il sentiment
def get_sentiment(words):
    text = " ".join(words)  # Convertire la lista di parole in una stringa
    score = sia.polarity_scores(text)
    return score['compound']

# Funzione per determinare se il sentiment è positivo o negativo
def sentiment_label(score):
    if score >= 0.3:
        result="positive" 
    elif score <= -0.3 :
        result="negative"
    else:
        result="neutral"

    return result


# Creare una UDF (User Defined Function) per il sentiment
sentiment_udf = udf(get_sentiment, FloatType())
label_udf = udf(sentiment_label, StringType())

# Applicare la UDF al DataFrame
spark_df = spark_df.withColumn("sentiment", sentiment_udf(spark_df["lemmatized_words"]))
spark_df = spark_df.withColumn("posORneg", label_udf(spark_df["sentiment"]))

# Mostrare il DataFrame risultante
spark_df.select("text_translated", "sentiment","posORneg").show(truncate=False)






